---
title: Multi-sample analyses
vignette: >
  % \VignetteIndexEntry{Multi-sample analysis}
  %\VignetteEncoding{UTF-8}
  % \VignetteEngine{knitr::rmarkdown}
output:
  html_document:
    mathjax: null   
editor_options: 
  chunk_output_type: console
---

# Setup and data exploration

As said, we will use the the data from the Tal1 chimera experiment:

- Sample 5: E8.5 injected cells (tomato positive), pool 3
- Sample 6: E8.5 host cells (tomato negative), pool 3
- Sample 7: E8.5 injected cells (tomato positive), pool 4
- Sample 8: E8.5 host cells (tomato negative), pool 4
- Sample 9: E8.5 injected cells (tomato positive), pool 5
- Sample 10: E8.5 host cells (tomato negative), pool 5

Note that this is a paired design in which for each biological replicate (pool 3, 4, and 5), we have both host and injected cells.

We start by loading the data and doing a quick exploratory analysis, essentially applying the normalization and visualization techniques that we have seen in the previous lectures to all samples.

```{r setup, message = FALSE}
library(MouseGastrulationData)
sce <- WTChimeraData(samples=5:10, type = "processed")
sce
colData(sce)
```

We now normalize the data and visualize them in a tSNE plot.

```{r}
library(scater)
library(ggplot2)
library(scran)

# remove doublets
drop <- sce$celltype.mapped %in% c("stripped", "Doublet")
sce <- sce[,!drop]

# normalization
sce <- logNormCounts(sce)

# identify highly variable genes
dec <- modelGeneVar(sce, block=sce$sample)
chosen.hvgs <- dec$bio > 0

# dimensionality reduction
sce <- runPCA(sce, subset_row = chosen.hvgs, ntop = 1000)
sce <- runTSNE(sce, dimred = "PCA")

sce$sample <- as.factor(sce$sample)
plotTSNE(sce, colour_by = "sample")
plotTSNE(sce, colour_by = "celltype.mapped") +
    scale_color_discrete() +
    theme(legend.position = "bottom")
```

There are evident sample effects. Depending on the analysis that you want to do you may want to remove or retain the sample effect. For instance, if the goal is to identify cell types with a clustering method, one may want to remove the sample effects with "batch effect" correction methods.

For now, let's assume that we want to remove this effect.

# Correcting batch effects

We correct the effect of samples by aid of the `correctExperiment` function
in the `batchelor` package and using the `sample` `colData` column as batch.


```{r}

## This needs work
library(batchelor)
set.seed(10102)
merged <- correctExperiments(sce, 
    batch=sce$sample, 
    subset.row=chosen.hvgs,
    PARAM=FastMnnParam(
        merge.order=list(
            list(1,3,5), # WT (3 replicates)
            list(2,4,6)  # td-Tomato (3 replicates)
        )
    )
)

merged <- runTSNE(merged, dimred="corrected")
plotTSNE(merged, colour_by="batch")

```

Once we removed the sample batch effect, we can proceed with the Differential 
Expression Analysis.


# Differential Expression

Section 4 of OSCA multisample.

In order to perform a Differential Expression Analysis, we need to identify 
group of cells across samples/conditions (depending on the experimental 
design and the final aim of the experiment). 

As previously saw, we have two ways of grouping cells, cell clustering and cell
labeling.
In our case we will focus on this secon aspect to group cells accordingly to the 
already annotated cell types to proceed with the computation of the 
pseudo-bulk samples.

## Pseudo-bulk samples

To compute differences between different group of cells, a possible way is to 
compute pseudo-bulk samples, where we mediate the gene signal of all the cells
for each specific cell type.
In this manner, we are then able to detect differences between the same cell type 
across two different condition, but also across different cell types in the same condition, etcetera.

To compute pseudo-bulk samples, we use `aggregateAcrossCells` function in the 
`scuttle` package, which takes as input not only the single cell experiment, 
but also the id to use for the identification of the group of cells.
In our case, we use as id not just the cell type, but also the sample, because
we want be able to discern between replicates and conditions during further steps.

```{r}
# Using 'label' and 'sample' as our two factors; each column of the output
# corresponds to one unique combination of these two factors.
library(scuttle)
summed <- aggregateAcrossCells(merged, 
    id=colData(merged)[,c("celltype.mapped", "sample")])
summed

```

## Differential Expression Analysis

The main advantage of using pseudo-bulk samples is the possibility to use 
well-tested methods for differential analysis like `edgeR` and `DESeq2`, we will
focus on the first one for this analysis.
`edgeR` uses a Negative Binomial Generalized Linear Modal for data with a limited 
number of replicates, like our case.

Before to begin our analysis, let's say that we are mostly interested in a specific
cell type, the "Mesenchymal stem cells" and look into differences across different
conditions.

<!-- To proceed with our analysis, we need to create an edgeR object for our data, so -->
<!-- we first subset our dataset for the specific cell type and then we create the object. -->

```{r}
label <- "Mesenchyme"
current <- summed[,label==summed$celltype.mapped]

# Creating up a DGEList object for use in edgeR:
library(edgeR)
y <- DGEList(counts(current), samples=colData(current))
y
```

A typical step is the discard of low quality samples due to low sequenced library 
size. We discard these samples because they can affect further steps like normalization
and/or DEGs analysis.

We can see that in our case we don't have low quality samples and we don't need 
to filter out any of them.

```{r}
discarded <- current$ncells < 10
y <- y[,!discarded]
summary(discarded)
```

The same idea is typically applied to the genes, indeed we need to discard low 
expressed genes to improve accuracy for the DEGs modeling.

We can see that in the case of the genes we discard 5688 of them, reducing the 
number of genes to test for further analysis to 9011.

```{r}
keep <- filterByExpr(y, group=current$tomato)
y <- y[keep,]
summary(keep)
```

Once we consider the data satisfying, we can proceed to normalize them.
There are several approaches for normalizing bulk, and so pseudo-bulk data, but
we stick with the Trimmed Mean of M-values method implemented in the `edgeR` 
package with the `calcNormFactors` function.
Keep in mind that because we are going to normalize the pseudo-bulk counts,
we don't need to normalize the data in "single cell form".

```{r}
y <- calcNormFactors(y)
y$samples
```

To investigate the effect of our normalization, we use an MD plot for each sample
in order to detect possible normalization problems due to insufficient cells/reads/UMIs composing a particular pseudo-bulk profile.

In our case, we verify that all these plots are centered in 0 (on y-axis) and present
a trumpet shape, as expected.


```{r}
par(mfrow=c(2,3))
for (i in seq_len(ncol(y))) {
    plotMD(y, column=i)
}
```

On the other hand, we want to understand if the samples weel-group together based
on their known factors (like the tomato injection in this case).

To do so, we use the MDS plot, which is very close to a PCA representation.
    
```{r}
par(mfrow=c(1,1))
plotMDS(cpm(y, log=TRUE), 
    col=ifelse(y$samples$tomato, "red", "blue"))
```

We then construct a matrix factor by including both the pool and the tomato as factors.
This design indicates which samples belong to which pool and condition, so we can
use it in the next step of the analysis.

```{r}
design <- model.matrix(~factor(pool) + factor(tomato), y$samples)
design
```

Now we can compute the Negative Binomial (NB) overdispersion parameter, to model
the mean-variance trend.

```{r}
y <- estimateDisp(y, design)
summary(y$trended.dispersion)
```

The BCV plot allows us to investigate the relation between the Biological Coefficient
of Variation and the Average log CPM for each gene.
Additionally, the Common and Trend BCV are shown in `red` and `blue`.

```{r}
plotBCV(y)
```


TO REPHRASE

This fits a Generalized Linear Model (GLM) to the counts for each gene and estimates 
the Quasi Likelihood (QL) dispersion from the GLM deviance, with `robust=TRUE` to
avoid distorsions from highly variable clusters. 
The QL dispersion models the uncertainty and variability of the per-gene variance which is not well handled by the NB dispersions, so the two dispersion types complement each other in the final analysis.


```{r}
fit <- glmQLFit(y, design, robust=TRUE)
summary(fit$var.prior)
summary(fit$df.prior)
```

TO REPHRASE
QL dispersion estimates for each gene as a function of abundance. Raw estimates (black) are shrunk towards the trend (blue) to yield squeezed estimates (red)

```{r}
plotQLDisp(fit)
```

We then use the QL-test function to test for Fold-Change (FC) differences 
(due to tomato injection) per each gene at a False Discovery Rate (FDR) of 5%.
The low amount of DGEs highlights that the tomato injection effect has a low 
influence on the mesenchyme cells.

```{r}
res <- glmQLFTest(fit, coef=ncol(design))
summary(decideTests(res))
topTags(res)
```

All the previous steps can be easily performed with the following function 
for each cell type, thanks to the `pseudoBulkDGE` function in the `scran` package.

```{r}
library(scran)
summed.filt <- summed[,summed$ncells >= 10]

library(scran)
de.results <- pseudoBulkDGE(summed.filt, 
    label=summed.filt$celltype.mapped,
    design=~factor(pool) + tomato,
    coef="tomatoTRUE",
    condition=summed.filt$tomato 
)
```

The returned object is a list of `DataFrame`s each with the results for a cell type.
Each of these contains also the intermediate results in `edgeR` format to perform
any intermediate plot or diagnostic.

```{r}
cur.results <- de.results[["Allantois"]]
cur.results[order(cur.results$PValue),]
```


# Differential Abundance

Section 6.3 of OSCA multisample.

<!-- from section 6.2-->

We first setup some code and variables for further analysis.

```{r}
library(edgeR)
abundances <- table(merged$celltype.mapped, merged$sample) 
abundances <- unclass(abundances) 
# Attaching some column metadata.
extra.info <- colData(merged)[match(colnames(abundances), merged$sample),]
y.ab <- DGEList(abundances, samples=extra.info)
keep <- filterByExpr(y.ab, group=y.ab$samples$tomato)
y.ab <- y.ab[keep,]
design <- model.matrix(~factor(pool) + factor(tomato), y.ab$samples)
y.ab <- estimateDisp(y.ab, design, trend="none")
```

## Assuming most labels do not change

```{r}
y.ab2 <- calcNormFactors(y.ab)
y.ab2$samples$norm.factors
y.ab2 <- estimateDisp(y.ab2, design, trend="none")
fit.ab2 <- glmQLFit(y.ab2, design, robust=TRUE, abundance.trend=FALSE)
res2 <- glmQLFTest(fit.ab2, coef=ncol(design))
topTags(res2, n=10)
```

##  Testing against a log-fold change threshold

```{r}
res.lfc <- glmTreat(fit.ab, coef=ncol(design), lfc=1)
summary(decideTests(res.lfc))
topTags(res.lfc)
```


# Session Info

```{r, tidy=TRUE}
sessionInfo()
```

